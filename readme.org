* Dont blame me blame my data
Identify a subset of the training data that makes a classifier
unfair. In this demo, we use Demographic Parity as our fairness
constraint, so unless $p(\hat{Y} | A = a) = p(\hat{Y} | A = a')
\forall a, a'$, then we declare our classifier as unfair. For
simplicity, we first consider the case with only a single binary
protected attribute.

We evaluate the distance between these two distributions using the KL
divergence: $$D_{KL} (P || Q) = \sum_y P(y) \log(\frac{P(y)}{Q(y)})$$
where $P(y) = p(\hat{Y} = y | A = 0)$ and $Q(y) = p(\hat{Y} = y | A =
1)$ were $A = 0$ indicates the protected group. We ues the KL
divergence as our metric for unfairness, the larger the distance
between these two distributions, the more unfair our classifier is.

We take two approaches to determining how each training example
affects our fairness on a test set.
1. Leave-one-out retraining of the model for each training point and evaluating the change in unfairness for each model.
2. Computing the influence of each training point on the unfairness
   over the test set. We follow the approach of [[https://arxiv.org/pdf/1703.04730.pdf][Koh and Liang]],
   extending it to the case where we evaluate influence of a training
   point on a criteria different from the loss function the model was
   trained on.

After evaluating the impact of each training point on our classifier's
fairness, we select a subset that is most responsible for the
unfairness exhibited on the test set. Similar to [[https://arxiv.org/pdf/1810.10118.pdf][Khanna et al.]], we
take advantage of the submodularity of this objective and greedily
insert training examples into this set.
** Results
| Method                     | Unfairness | Accuracy |
|----------------------------+------------+----------|
| Original                   | 5.5793e-06 |   0.8154 |
| Retrain, drop top 100      |         ?? |       ?? |
| Influence, drop top 100    | 1.4734e-07 |   0.8149 |
| Influence, drop bottom 100 |     1.0e-4 |   0.8166 |
